                                                   ASSIGNMENT – 39 
                                                   MACHINE LEARNING
--------------------------------------------------------------------------------------------------------------------------------------------
1. The computational complexity of linear regression is:
A) 𝑂(𝑛2.4) B) 𝑂(𝑛) C) 𝑂(𝑛2) D) 𝑂(𝑛3)

Ans-   C) 𝑂(𝑛2)
--------------------------------------------------------------------------------------------------------------------------------------------
2. Which of the following can be used to fit non-linear data?
A) Lasso Regression B) Logistic Regression
C) Polynomial Regression D) Ridge Regression

Ans-   C) Polynomial Regression
--------------------------------------------------------------------------------------------------------------------------------------------
3. Which of the following can be used to optimize the cost function of Linear Regression? 
A) Entropy B) Gradient Descent
C) Pasting D) None of the above.

Ans-   B) Gradient Descent
--------------------------------------------------------------------------------------------------------------------------------------------
4. Which of the following method does not have closed form solution for its coefficients?
A) extrapolation B) Ridge
C) Lasso D) Elastic Nets

Ans-   C) Lasso
--------------------------------------------------------------------------------------------------------------------------------------------
5. Which gradient descent algorithm always gives optimal solution?
A) Stochastic Gradient Descent B) Mini-Batch Gradient Descent
C) Batch Gradient Descent D) All of the above

Ans-   D) All of the above
--------------------------------------------------------------------------------------------------------------------------------------------
6. Generalization error measures how well a model performs on training data.
A) True B) False

Ans-   A) True
--------------------------------------------------------------------------------------------------------------------------------------------
7. The cost function of linear regression can be given as 𝐽(𝑤0, 𝑤1) =12𝑚∑ (𝑤0 + 𝑤1𝑥(𝑖) − 𝑦(𝑖))𝑚 2𝑖=1. 
The half term at start is due to:
A) scaling cost function by half makes gradient descent converge faster.
B) presence of half makes it easy to do grid search.
C) it does not matter whether half is there or not.
D) None of the above.

Ans-   A) scaling cost function by half makes gradient descent converge faster.
--------------------------------------------------------------------------------------------------------------------------------------------
8. Which of the following will have symmetric relation between dependent variable and independent 
variable?
A) Regression B) Correlation
C) Both of them D) None of these

Ans-    B) Correlation
--------------------------------------------------------------------------------------------------------------------------------------------
9. Which of the following is true about Normal Equation used to compute the coefficient of the Linear 
Regression?
A) We don’t have to choose the learning rate.
B) It becomes slow when number of features are very large.
C) We need to iterate.
D) It does not make use of dependent variable.

Ans-    A) We don’t have to choose the learning rate.
--------------------------------------------------------------------------------------------------------------------------------------------
10. Which of the following statement/s are true if we generated data with the help of polynomial features 
with 5 degrees of freedom which perfectly fits the data?
A) Linear Regression will have high bias and low variance.
B) Linear Regression will have low bias and high variance.
C) Polynomial with degree 5 will have low bias and high variance.
D) Polynomial with degree 5 will have high bias and low variance. MACHINE LEARNING

Ans-    A) Linear Regression will have high bias and low variance.
--------------------------------------------------------------------------------------------------------------------------------------------
11. Which of the following sentence is false regarding regression?
A) It relates inputs to outputs. 
B) It is used for prediction.
C) It discovers causal relationship. 
D) No inference can be made from regression line

Ans-    C) It discovers causal relationship. 

--------------------------------------------------------------------------------------------------------------------------------------------

12. Which Linear Regression training algorithm can we use if we have a training set with millions of features?

Ans- We could use batch gradient descent, stochastic gradient descent, or mini-batch gradient descent. SGD and MBGD can work better because they don't need to load the entire dataset into memory in order to take 1 step of gradient descent.

--------------------------------------------------------------------------------------------------------------------------------------------

13. Which algorithms will not suffer or might suffer, if the features in training set have very different scales?

Ans- If the features in training set have varied scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. so to solve this we should scale the data before training the model.

--------------------------------------------------------------------------------------------------------------------------------------------

                                              submitted by- Ayush chaurasiya
                                                       Thank you